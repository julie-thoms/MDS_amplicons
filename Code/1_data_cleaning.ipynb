{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Integrating data from original run (run 1) and CUX1 rescue from run 2\n",
    "#File anon_allele_counts_resc_2 contains reads counts from all plates \n",
    "#using 40bp to identify amplicon and 100bp to call mutations (except for CUX1 and JP001 TET2b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the data\n",
    "\n",
    "sourcefile = '../Data/Amp_data/anon_allele_counts_resc_2.tsv'\n",
    "\n",
    "#Import the data\n",
    "df = pd.read_csv(sourcefile, header = [0,1,2], index_col = 0, sep='\\t')\n",
    "df = df.stack([0,1])\n",
    "df['Plate'] = df.index.get_level_values(1)\n",
    "df['Well'] = df.index.get_level_values(0)\n",
    "df['Amplicon'] = df.index.get_level_values(2)\n",
    "df[['Patient', 'one', 'two']] = df['Amplicon'].str.split('_', expand = True)\n",
    "df = df.drop(columns = ['one', 'two'])\n",
    "\n",
    "#Create a dictionary to rename subscripted plates back to original name\n",
    "#Plates in new run were given extra letter to distinguish them (a, plus b, c, d for AS-206 only with rescue of additional amplicons)\n",
    "subs = ['a', 'b', 'c', 'd']\n",
    "plate_key = df.loc[(df['Plate'].str.contains('a')) | (df['Plate'].str.contains('b')) | (df['Plate'].str.contains('c')) | (df['Plate'].str.contains('d'))]['Plate'].drop_duplicates().to_list()\n",
    "plate_values = []\n",
    "for p in plate_key:\n",
    "    for s in subs:\n",
    "        if s in p:\n",
    "            q = p.split(s)[0]\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    plate_values.append(q)\n",
    "\n",
    "plate_rename = dict(zip(plate_key, plate_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\z3283497\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:6746: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PD7151 6912 ['PD7151_TET2a', 'PD7151_TET2b']\n",
      "JP001 20736 ['JP001_RUNX1_c', 'JP001_RUNX1_g', 'JP001_SRSF2', 'JP001_TET2a', 'JP001_TET2b_c', 'JP001_TET2b_g']\n",
      "PD7153 20736 ['PD7153_CUX1', 'PD7153_SRSF2', 'PD7153_TET2a', 'PD7153_TET2b', 'PD7153_TGFB3_c', 'PD7153_TGFB3_g']\n",
      "Expected number of rows =  48384 Actual number of rows =  48384\n",
      "Any duplicate rows? (array([], dtype=int64),)\n"
     ]
    }
   ],
   "source": [
    "#Select appropriate data source (plates given an extra letter in run2 so they can be distinguished)\n",
    "#Expected number of rows (n) = (2 x PD7151 + 6 x PD7153 + 6 x JP001 Amplicons) x 9 plates x 384 wells\n",
    "n = (2 + 6 + 6) * 9 * 384\n",
    "\n",
    "#PD 7151 - TET2a and TET2b reads come from run 1\n",
    "df1 = df.loc[(df['Patient'].isin(['PD7151'])) & (~df['Plate'].str.contains('a')) & (df['Amplicon'].isin(['PD7151_TET2a', 'PD7151_TET2b']))]\n",
    "df1['Plate'].replace(plate_rename, inplace = True)  #rename plates\n",
    "print('PD7151' , df1.shape[0], df1['Amplicon'].drop_duplicates().to_list())\n",
    "\n",
    "\n",
    "#JP001 - all reads come from run 1\n",
    "\n",
    "df2 = df.loc[(df['Patient'].isin(['JP001'])) & (~df['Plate'].str.contains('a'))] \n",
    "df2['Plate'].replace(plate_rename, inplace = True)  #rename plates\n",
    "print('JP001', df2.shape[0], df2['Amplicon'].drop_duplicates().to_list())\n",
    "\n",
    "#PD7153 - CUX1 reads from run 1 replaced with CUX1 from rescue\n",
    "\n",
    "#CUX1 from run2\n",
    "a = df.loc[(df['Patient'].isin(['PD7153'])) & (df['Plate'].str.contains('a')) & (df['Amplicon'].isin(['PD7153_CUX1']))]\n",
    "#run1 except any rescue data\n",
    "b = df.loc[(df['Patient'].isin(['PD7153'])) & (~df['Amplicon'].str.contains('PD7153_CUX1')) & (~df['Plate'].str.contains('a')) & (~df['Plate'].str.contains('b')) & (~df['Plate'].str.contains('c')) & (~df['Plate'].str.contains('d'))]\n",
    "           \n",
    "df3 = pd.concat([a, b])\n",
    "df3['Plate'].replace(plate_rename, inplace = True) #rename plates\n",
    "print('PD7153', df3.shape[0], df3['Amplicon'].drop_duplicates().to_list())\n",
    "\n",
    "#Put everything back together and fix the indexes, then put into format to match existing code and save to csv\n",
    "df_all = pd.concat([df1, df2, df3])\n",
    "print('Expected number of rows = ', n, 'Actual number of rows = ', df_all.shape[0])\n",
    "df_all.rename(index = plate_rename, inplace = True)\n",
    "df_all.drop(columns = ['Plate', 'Well', 'Amplicon', 'Patient'], inplace = True)\n",
    "df_all = df_all.stack()\n",
    "df_all = df_all.reorder_levels([1, 0, 2, 3])\n",
    "df_all = df_all.to_frame()\n",
    "df_all.columns = ['Reads']\n",
    "df_all.to_csv('../Data/Amp_data/clean_anon_allele_counts_resc_2.tsv', sep = '\\t')\n",
    "\n",
    "print('Any duplicate rows?', np.where(df_all.index.duplicated()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
